{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = \"Make a Clasifier to Predict Transcription Factor Binding Sites\"    \n",
    "# author = \"Cindy Pino-Barrios\"\n",
    "# date = \"03-22-2023\"\n",
    "# description = \"Transcription factors are proteins that bind DNA at promoters to drive gene expression. Most preferentially bind to specific sequences while ignoring others. Traditional methods to determine these sequences (called motifs) have assumed that binding sites in the genome are all independent. However, in some cases people have identified motifs where positional interdependencies exist.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Clasifier to Predict Transcription Factor Binding Sites\n",
    "\n",
    "You will implement a multi-layer fully connected neural network using your `NeuralNetwork` class to predict whether a short DNA sequence is a binding site for the yeast transcription factor Rap1. The training data is incredibly imbalanced, with way fewer positive sequences than negative sequences, so you will implement a sampling scheme to ensure that class imbalance does not affect training. As in step 2, all of the following work should be done in a Jupyter Notebook.\n",
    "\n",
    "### To-do\n",
    "\n",
    "* Use the `read_text_file` function from `io.py` to read in the 137 positive Rap1 motif examples.\n",
    "* Use the `read_fasta_file` function from `io.py` to read in all the negative examples. Note that these sequences are much longer than the positive sequences, so you will need to process them to the same length.\n",
    "* Balance your classes using your `sample_seq` function and explain why you chose the sampling scheme you did.\n",
    "* One-hot encode the data using your `one_hot_encode_seqs` function.\n",
    "* Split the data into training and validation sets.\n",
    "* Generate an instance of your `NeuralNetwork` class with an appropriate architecture.\n",
    "* Train your neural network on the training data.\n",
    "* Plot your training and validation loss by epoch.\n",
    "* Report the accuracy of your classifier on your validation dataset.\n",
    "* Explain your choice of loss function and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages and modules \n",
    "\n",
    "from nn.nn import NeuralNetwork\n",
    "from nn.preprocess import sample_seqs, one_hot_encode_seqs\n",
    "from nn.io import read_text_file, read_fasta_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Process the data, \n",
    "\n",
    "- Read the positives and negatives using io read_text_file and read_fasta_file functions\n",
    "- Generate a labels parameter where 1 corresponds to positive and 0 corresponds to negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the read_text_file function to read in the data from the file \"data/rap1-lieb-positives.txt\"\n",
    "# and store the result in the variable pos_seqs\n",
    "\n",
    "pos_seqs = read_text_file(\"data/rap1-lieb-positives.txt\")\n",
    "\n",
    "# Use the read_fasta_file function to read in the data from the file \"data/yeast-upstream-1k-negative.fa\"\n",
    "# and store the result in the variable neg_seqs\n",
    "\n",
    "neg_seqs = read_fasta_file(\"data/yeast-upstream-1k-negative.fa\")\n",
    "\n",
    "# Use the sample_seqs function to sample the negative sequences to account for class imbalance\n",
    "# and store the result in the variables neg_seqs_sampled and neg_labels_sampled\n",
    "\n",
    "# Generate a list to store processed sequences \n",
    "neg_seqs_sampled = []\n",
    "\n",
    "# Positive sequences are shorter than negative sequences, so we need to sample the negative sequences to account for class imbalance\n",
    "\n",
    "post_seqs_len = len(pos_seqs[0])\n",
    "\n",
    "# For the negative sequences, I will iterate through the list and only add the sequences that are the same length as the positive sequences and then add them to the negatve_seqs_sampled list\n",
    "\n",
    "for seq in neg_seqs:\n",
    "    for i in range(len(seq) - post_seqs_len + 1):\n",
    "        if len(seq[i:i+post_seqs_len]) == post_seqs_len:\n",
    "            neg_seqs_sampled.append(seq[i:i+post_seqs_len])\n",
    "\n",
    "\n",
    "# Combine the positive and negative sequences into one list\n",
    "seqs = pos_seqs + neg_seqs_sampled\n",
    "\n",
    "# Generate a list to store the positive labels for the sequences\n",
    "pos_labels = []\n",
    "\n",
    "# For the positive sequences, I will add a 1 to the labels list\n",
    "# len of positive labels should be the same as the length of the positive sequences\n",
    "\n",
    "for seq in pos_seqs:\n",
    "    pos_labels.append(1)\n",
    "\n",
    "\n",
    "# Generate a list to store the negative labels for the sequences\n",
    "neg_labels = []\n",
    "\n",
    "# For the negative sequences, I will add a 0 to the labels list\n",
    "# len of negative labels should be the same as the length of the negative sequences\n",
    "\n",
    "for seq in neg_seqs_sampled:\n",
    "    neg_labels.append(0)\n",
    "\n",
    "# Combine the positive and negative labels into one list\n",
    "\n",
    "labels = pos_labels + neg_labels\n",
    "\n",
    "assert len(seqs) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'sample_seqs' function \n",
    "# to sample the negative sequences to account for class imbalance\n",
    "# and store the result in seqs and labels\n",
    "\n",
    "seqs, labels = sample_seqs(seqs, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the one_hot_encode_seqs function to one-hot encode the sequences and store the result in the variables X and y\n",
    "# use the neg_seqs_sampled and neg_labels_sampled variables as inputs to the function\n",
    "\n",
    "X = one_hot_encode_seqs(seqs) # One-hot encode the sequences\n",
    "y = np.array(labels, dtype=int) # Convert the labels to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3109826, 68), (3109826,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nn_arch: List[Dict[str, Union[int, str]]],\n",
    "                 lr: float, seed: int, batch_size: int, epochs: int, loss_function: str):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "    \n",
    "    def _single_forward(self, W_curr: ArrayLike, b_curr: ArrayLike,\n",
    "                        A_prev: ArrayLike, activation: str) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set current layer Z matrix to be the weights matrix multiplied by the previous layer's activation matrix plus the bias matrix\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "        # Set current layer activation matrix to be the activation function of the current layer Z matrix\n",
    "        # If the activation function is relu, use the relu function \n",
    "        if activation == 'relu':\n",
    "            A_curr = self._relu(Z_curr)\n",
    "        # If the activation function is sigmoid, use the sigmoid function\n",
    "        elif activation == 'sigmoid':\n",
    "            A_curr = self._sigmoid(Z_curr)\n",
    "        # If neither relu nor sigmoid, raise an exception\n",
    "        else:\n",
    "            raise Exception('Non-supported activation function. Please specify either \"relu\" or \"sigmoid\" as activation functions.')\n",
    "        \n",
    "        return A_curr, Z_curr # return Tuple[ArrayLike, ArrayLike] of current layer activation matrix and current layer linear transformed matrix\n",
    "    \n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize cache dictionary\n",
    "        cache = {'A0': X}\n",
    "\n",
    "        # Set current layer's activation matrix to be the input matrix\n",
    "        A_curr = X\n",
    "\n",
    "        # Loop through each layer in the neural network\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            \n",
    "            # Set current layer's activation matrix to be the previous layer's activation matrix\n",
    "            A_prev = A_curr\n",
    "\n",
    "            # Get index of first layer (layer 1)\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            # Extract corresponding weight matrix and bias matrix for current layer from parameter dictionary\n",
    "            W_curr = self._param_dict['W' + str(layer_idx)]\n",
    "            b_curr = self._param_dict['b' + str(layer_idx)]\n",
    "            activation = layer['activation']\n",
    "\n",
    "            # Perform a single forward pass on the current layer\n",
    "            A_curr, Z_curr = self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "\n",
    "            # Store current layer's Z matrix and A matrix in cache\n",
    "            cache['Z' + str(layer_idx)] = Z_curr\n",
    "            cache['A' + str(layer_idx)] = A_curr\n",
    "\n",
    "        # Set output to be the final layer's activation matrix\n",
    "        output = A_curr.T\n",
    "\n",
    "        return output, cache\n",
    "    \n",
    "    def _single_backprop(self, W_curr: ArrayLike, b_curr: ArrayLike, Z_curr: ArrayLike, A_prev: ArrayLike,\n",
    "                         dA_curr: ArrayLike, activation_curr: str) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the activation function is relu, use the relu derivative function\n",
    "        if activation_curr == 'relu':\n",
    "            activation_func = self._relu_backprop(dA_curr, Z_curr)\n",
    "\n",
    "        # If the activation function is sigmoid, use the sigmoid derivative function\n",
    "        elif activation_curr == 'sigmoid':\n",
    "            activation_func = self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "\n",
    "        # If neither relu nor sigmoid, raise an exception\n",
    "        else:\n",
    "            raise Exception('Non-supported activation function. Please specify either \"relu\" or \"sigmoid\" as activation functions.')\n",
    "        \n",
    "        # Set current layer's partial derivative of loss function with respect to current layer's Z matrix to be the activation function derivative multiplied by the current layer's partial derivative of loss function with respect to current layer's activation matrix\n",
    "        dZ_curr = activation_func\n",
    "\n",
    "        # Compute the gradient for the current layer\n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T) / np.shape(A_prev)[1]\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / np.shape(A_prev)[1]\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr # return Tuple[ArrayLike, ArrayLike, ArrayLike] of partial derivative of loss function with respect to previous layer activation matrix, partial derivative of loss function with respect to current layer weight matrix, and partial derivative of loss function with respect to current layer bias matrix\n",
    "    \n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize gradient dictionary\n",
    "        grad_dict = {}\n",
    "\n",
    "        # Set the partial derivative of loss function with respect to final layer activation matrix to be the difference between the predicted output and the ground truth labels\n",
    "        if self._loss_func == 'mse':\n",
    "            dA_prev = self._mean_squared_error_backprop(y, y_hat)\n",
    "        elif self._loss_func == 'bce':\n",
    "            dA_prev = self._binary_cross_error_backprop(y, y_hat)\n",
    "        else:\n",
    "            raise Exception('Non-supported loss function. Please specify either \"mse\" or \"bce\" as loss functions.')\n",
    "\n",
    "        # Loop through each layer in the neural network\n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.arch))):\n",
    "\n",
    "            # Get index of current layer\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            # Extract corresponding activation matrix and Z matrix for current layer from cache\n",
    "            if layer_idx_prev == 0:\n",
    "                A_prev = cache['A0']\n",
    "            else:\n",
    "                A_prev = cache['A' + str(layer_idx_prev)]\n",
    "\n",
    "            # Extract corresponding weight matrix and bias matrix for current layer from parameter dictionary\n",
    "            W_curr = self._param_dict['W' + str(layer_idx_curr)]\n",
    "            b_curr = self._param_dict['b' + str(layer_idx_curr)]\n",
    "            Z_curr = cache['Z' + str(layer_idx_curr)]\n",
    "\n",
    "            # Extract activation function for current layer\n",
    "            activation_curr = layer['activation']\n",
    "\n",
    "            # Perform a single backprop pass on the current layer\n",
    "            dA_prev, dW_curr, db_curr = self._single_backprop(W_curr, b_curr, Z_curr, A_prev, dA_prev, activation_curr)\n",
    "\n",
    "            # Store current layer's partial derivative of loss function with respect to current layer's weight matrix, current layer's partial derivative of loss function with respect to current layer's bias matrix, and previous layer's partial derivative of loss function with respect to previous layer's activation matrix in gradient dictionary\n",
    "            grad_dict['dW' + str(layer_idx_curr)] = dW_curr\n",
    "            grad_dict['db' + str(layer_idx_curr)] = db_curr\n",
    "            grad_dict['dA' + str(layer_idx_prev)] = dA_prev\n",
    "\n",
    "        return grad_dict\n",
    "    \n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop through each layer in the neural network and update the parameters for W and b\n",
    "        for layer_idx, layer in enumerate(self.arch, 1):\n",
    "            self._param_dict['W' + str(layer_idx)] -= self._lr * grad_dict['dW' + str(layer_idx)]\n",
    "            self._param_dict['b' + str(layer_idx)] -= self._lr * grad_dict['db' + str(layer_idx)]\n",
    "\n",
    "    def fit(self, X_train: ArrayLike, y_train: ArrayLike,\n",
    "            X_val: ArrayLike, y_val: ArrayLike) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize lists of training and validation losses\n",
    "        per_epoch_loss_train = []\n",
    "        per_epoch_loss_val = []\n",
    "\n",
    "        # Iterate across each epoch\n",
    "\n",
    "\n",
    "        for epoch in range((self._epochs)):\n",
    "            # Shuffle indices to randomly select batches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            # Initialize list to keep track of different batches\n",
    "            batches = []\n",
    "\n",
    "            # Iterate through rows of X_train in increments of batch size\n",
    "            for i in range(0, X_train.shape[0], self._batch_size):\n",
    "                X_batch = X_train[i:i + self._batch_size]  # Use batch indices to select corresponding rows from X_train\n",
    "                y_batch = y_train[i:i + self._batch_size]  # Use batch indices to select corresponding rows from y_train\n",
    "                batches.append((X_batch, y_batch))  # Add the batch to the list of batches\n",
    "\n",
    "            # Iterate through the X and y batches\n",
    "            for X_batch, y_batch in batches:\n",
    "                X_batch = X_batch.T  # Transpose each X batch\n",
    "                y_batch = y_batch.T  # Transpose each X batch\n",
    "                y_hat, cache = self.forward(X_batch)  # Forward pass on X batches\n",
    "                grad_dict = self.backprop(y_batch, y_hat, cache)  # Backpropagate using y_batches\n",
    "                self._update_params(grad_dict)  # Update parameters using values stored from backpropagation\n",
    "\n",
    "            # Calculate predictions from training and validation sets\n",
    "            y_hat_train = self.predict(X_train)\n",
    "            y_hat_val = self.predict(X_val)\n",
    "\n",
    "            # Calculate training and validation losses using user-defined loss function\n",
    "            if \"bce\" in self._loss_func:\n",
    "                train_loss = self._binary_cross_error(y_train.T, y_hat_train.T)\n",
    "                val_loss = self._binary_cross_error(y_val.T, y_hat_val.T)\n",
    "            elif \"mse\" in self._loss_func:\n",
    "                train_loss = self._mean_squared_error(y_train.T, y_hat_train.T)\n",
    "                val_loss = self._mean_squared_error(y_val.T, y_hat_val.T)\n",
    "            else:\n",
    "                raise NameError(\"Loss function name is not defined. \"\n",
    "                                \"Choose either mean squared error or binary cross entropy as loss function.\")\n",
    "\n",
    "            # Append calculated loss to per epoch loss lists\n",
    "            per_epoch_loss_train.append(train_loss)\n",
    "            per_epoch_loss_val.append(val_loss)\n",
    "\n",
    "        return per_epoch_loss_train, per_epoch_loss_val\n",
    "                \n",
    "    \n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        This function makes predictions for the input features.\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input features.\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Perform forward pass\n",
    "        y_hat, _ = self.forward(X.T)\n",
    "    \n",
    "        return y_hat\n",
    "    \n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "\n",
    "        nl_transform = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "        return nl_transform\n",
    "    \n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        sig = self._sigmoid(Z)\n",
    "        dZ = dA * sig * (1 - sig)\n",
    "    \n",
    "        return dZ\n",
    "    \n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "    \n",
    "        return dZ\n",
    "    \n",
    "    def _binary_cross_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = -(1 / y.shape[1] * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)))\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def _binary_cross_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        dA = - (np.divide(y, y_hat.T) - np.divide((1 - y), (1 - y_hat.T)))\n",
    "        return dA\n",
    "    \n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        loss = (1 / (2 * y.shape[0]) * np.sum(np.square(y_hat - y)))\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        # Derivative of the MSE loss function\n",
    "\n",
    "        dA = (2 / y.shape[0] * y_hat.T - y)\n",
    "\n",
    "   \n",
    "        return dA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Autoencoder on TF data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2487860, 68), (621966, 68), (2487860,), (621966,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NeuralNetwork object\n",
    "\n",
    "# Parameters\n",
    "nn_arch = [{'input_dim': 68, 'output_dim': 32, 'activation': 'relu'},\n",
    "              {'input_dim': 32, 'output_dim': 16, 'activation': 'relu'},\n",
    "                {'input_dim': 16, 'output_dim': 4, 'activation': 'sigmoid'},    \n",
    "                {'input_dim': 4, 'output_dim': 1, 'activation': 'sigmoid'}]\n",
    "\n",
    "lr = 0.01\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "loss_func = 'mse'\n",
    "\n",
    "\n",
    "\n",
    "# Create a NeuralNetwork object\n",
    "nn = NeuralNetwork(nn_arch, lr, seed, batch_size, epochs, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = nn.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss by epoch\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Error: 0.000809867988613527\n",
      "Average Validation Error: 0.0008132377470897109\n"
     ]
    }
   ],
   "source": [
    "# print the average reconstruction error over the validation set\n",
    "print(f'Average Training Error: {np.mean(train_loss)}')\n",
    "print(f'Average Validation Error: {np.mean(val_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample seqs and one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two function calls to sample_seqs, one for the positive sequences and one for the negative sequences. \n",
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "def sample_seqs(seqs: List[str], labels: List[bool]) -> Tuple[List[str], List[bool]]:\n",
    "    \"\"\"\n",
    "    This function should sample the given sequences to account for class imbalance. \n",
    "    Consider this a sampling scheme with replacement.\n",
    "    \n",
    "    Args:\n",
    "        seqs: List[str]\n",
    "            List of all sequences.\n",
    "        labels: List[bool]\n",
    "            List of positive/negative labels\n",
    "\n",
    "    Returns:\n",
    "        sampled_seqs: List[str]\n",
    "            List of sampled sequences which reflect a balanced class size\n",
    "        sampled_labels: List[bool]\n",
    "            List of labels for the sampled sequences\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Get the number of positive and negative labels\n",
    "    num_pos = sum(labels)\n",
    "    num_neg = len(labels) - num_pos\n",
    "\n",
    "    # Get the number of sequences to sample\n",
    "    num_to_sample = min(num_pos, num_neg)\n",
    "\n",
    "    # Create a list to store the sampled sequences\n",
    "    sampled_seqs = []\n",
    "\n",
    "    # Create a list to store the sampled labels\n",
    "\n",
    "    sampled_labels = []\n",
    "\n",
    "    # Iterate through the sequences and labels\n",
    "    for seq, label in zip(seqs, labels):\n",
    "        # If the label is positive and the number of positive samples is less than the number of samples to take\n",
    "        if label and num_pos > 0:\n",
    "            # Add the sequence and label to the sampled sequences and labels\n",
    "            sampled_seqs.append(seq)\n",
    "            sampled_labels.append(label)\n",
    "            # Decrement the number of positive samples\n",
    "            num_pos -= 1\n",
    "        # If the label is negative and the number of negative samples is less than the number of samples to take\n",
    "        elif not label and num_neg > 0:\n",
    "            # Add the sequence and label to the sampled sequences and labels\n",
    "            sampled_seqs.append(seq)\n",
    "            sampled_labels.append(label)\n",
    "            # Decrement the number of negative samples\n",
    "            num_neg -= 1\n",
    "\n",
    "    assert len(sampled_seqs) == len(sampled_labels)\n",
    "\n",
    "    # Return the sampled sequences and labels\n",
    "\n",
    "      \n",
    "    return sampled_seqs, sampled_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the one_hot_encode_seqs function to one-hot encode the positive and negative sequences\n",
    "\n",
    "def one_hot_encode_seqs(seq_arr: List[str]) -> ArrayLike:\n",
    "    \"\"\"\n",
    "    This function generates a flattened one-hot encoding of a list of DNA sequences\n",
    "    for use as input into a neural network.\n",
    "\n",
    "    Args:\n",
    "        seq_arr: List[str]\n",
    "            List of sequences to encode.\n",
    "\n",
    "    Returns:\n",
    "        encodings: ArrayLike\n",
    "            Array of encoded sequences, with each encoding 4x as long as the input sequence.\n",
    "            For example, if we encode:\n",
    "                A -> [1, 0, 0, 0]\n",
    "                T -> [0, 1, 0, 0]\n",
    "                C -> [0, 0, 1, 0]\n",
    "                G -> [0, 0, 0, 1]\n",
    "            Then, AGA -> [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0].\n",
    "    \"\"\"\n",
    "    # Create a dictionary to map each nucleotide to a one-hot encoding\n",
    "\n",
    "    nucleotide_dict = {\"A\": [1, 0, 0, 0], \n",
    "                       \"T\": [0, 1, 0, 0], \n",
    "                       \"C\": [0, 0, 1, 0], \n",
    "                       \"G\": [0, 0, 0, 1]}\n",
    "\n",
    "    # Create a list to store the one-hot encodings\n",
    "    encodings = []\n",
    "    \n",
    "    # Loop through each sequence in the list of sequences\n",
    "\n",
    "    for seq in seq_arr:\n",
    "        # Create a list to store the one-hot encoding for the current sequence\n",
    "        seq_encoding = []\n",
    "\n",
    "        # Loop through each nucleotide in the sequence\n",
    "        for nucleotide in seq:\n",
    "            # Append the one-hot encoding for the current nucleotide to the current sequence encoding\n",
    "            seq_encoding.extend(nucleotide_dict[nucleotide])\n",
    "\n",
    "        # Append the current sequence encoding to the list of encodings\n",
    "        encodings.append(seq_encoding)\n",
    "\n",
    "    return np.array(encodings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BMI_203",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
